{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_35424\\226579981.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotebook\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTensorDataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSequentialSampler\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mField\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torchtext.legacy.data import Field\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_vocab(X):\n",
    "    words = [sentence.split() for sentence in X]\n",
    "    text_field = Field()\n",
    "    text_field.build_vocab(words, max_size=10000)\n",
    "    return text_field\n",
    "\n",
    "def pad(seq, maxlen):\n",
    "    if len(seq) < maxlen:\n",
    "        seq = seq + ['<pad>'] * (maxlen - len(seq))\n",
    "    return seq\n",
    "\n",
    "def to_indices(vocab, words):\n",
    "    return [vocab.stoi[w] for w in words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(X, y, model = 'simple', field=None, teacher_output = None, tokenizer = None):\n",
    "    X_split = [t.split() for t in X]\n",
    "    X_pad = [pad(s, maxlen) for s in X_split]\n",
    "\n",
    "    if model == 'bert':\n",
    "        lines = [\" \".join(s) for s in X_pad]\n",
    "        masks = [[int(word != '<pad>') for word in sentence] for sentence in X_pad]\n",
    "        inds = [tokenizer.encode(line.split(), add_special_tokens=False) for line in lines]\n",
    "        inds = torch.tensor(inds)\n",
    "        masks = torch.tensor(masks, dtype=torch.int8)\n",
    "        torch_y = torch.tensor(y, dtype=torch.float)\n",
    "        return TensorDataset(inds, torch_y, masks)\n",
    "\n",
    "    else:\n",
    "        X_index = [to_indices(field.vocab, s) for s in X_pad]\n",
    "        torch_x = torch.tensor(X_index, dtype=torch.long)\n",
    "        torch_y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "        if model == 'distil':\n",
    "            return TensorDataset(torch_x, torch_y, teacher_output)\n",
    "        else:\n",
    "            return TensorDataset(torch_x, torch_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_and_preprocess(path):\n",
    "    X, y, maxlen = [], [], 0\n",
    "    with open(sources_path + path, encoding = \"ISO-8859-1\") as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            y.append(0 if words[0] == 'ham' else 1)\n",
    "            X.append(' '.join(words[1:]))\n",
    "            maxlen = max(maxlen, len(words))\n",
    "    return X, y, maxlen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DistilLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(DistilLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, real_prediction, real_output, teacher_prediction, teacher_output):\n",
    "        bce = nn.CrossEntropyLoss()\n",
    "        mse = nn.MSELoss()\n",
    "        prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
    "        teacher_loss = mse(teacher_prediction, teacher_output)\n",
    "        return self.alpha * prediction_loss + (1 - self.alpha) * teacher_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_teacher_output(teacher, dataset):\n",
    "    teacher = teacher.to(device)\n",
    "    dataloader = DataLoader(dataset, 32, shuffle=False)\n",
    "    teacher_output = []\n",
    "    with torch.no_grad():\n",
    "      for info in tqdm(dataloader):\n",
    "          info=[t.to(device) for t in info]\n",
    "          loss, result = teacher(info)\n",
    "          teacher_output.append(result)\n",
    "      teacher_output = torch.cat(teacher_output)\n",
    "    return teacher_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \\\n",
    "                bidirectional, dropout, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "                            input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout\n",
    "                        )\n",
    "\n",
    "        self.label_prediction = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim), \\\n",
    "               torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        x, hidden = self.rnn(x)\n",
    "        hidden, cell = hidden\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        label_prediction = self.label_prediction(hidden)\n",
    "        return label_prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_labels=2):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, teacher, head_hidden_size=128):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        hidden_size = self.teacher.config.hidden_size\n",
    "        self.classification_head = ClassificationHead(hidden_size, head_hidden_size, 2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        masks = inp[2]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        output = self.teacher(inds, attention_mask=masks)[0]\n",
    "        output = output[:, 0, :]\n",
    "        prediction = self.classification_head(output)\n",
    "        loss = self.loss(prediction, labels)\n",
    "        return loss, prediction\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.classification_head.parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, bilstm):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.bilstm = bilstm\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        prediction = self.bilstm(inds)\n",
    "        loss = self.loss(prediction, labels)\n",
    "        return loss, prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DistilModel(nn.Module):\n",
    "    def __init__(self, student, alpha=0.5):\n",
    "        super(DistilModel, self).__init__()\n",
    "        self.student = student\n",
    "        self.loss = DistilLoss(alpha)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        teacher_output = inp[2]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        label_prediction = self.student(inds)\n",
    "        loss = self.loss(label_prediction, labels, label_prediction, teacher_output)\n",
    "        return loss, label_prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, dataset, test_dataset, epochs=5, batch_size=64):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        losses = 0\n",
    "        count = 0\n",
    "        for info in tqdm(dataloader):\n",
    "            info=[t.to(device) for t in info]\n",
    "            loss, _ = model(info)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            count += 1\n",
    "        losses /= count\n",
    "        print(f'Epoch {e} \\t| Loss: {losses.item()}')\n",
    "\n",
    "    dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for info in tqdm(dataloader):\n",
    "          info=[t.to(device) for t in info]\n",
    "          _, prediction = model(info)\n",
    "          prediction = torch.argmax(prediction, axis=1)\n",
    "          correct += torch.sum(prediction == info[1].int())\n",
    "          count += batch_size\n",
    "    print(f'Accuracy:{correct / count}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y, maxlen = read_and_preprocess('/spam.csv')\n",
    "field = get_vocab(X)\n",
    "vocab_size = len(field.vocab.stoi.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "teacher = TeacherModel(bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_dataset_train = tokenize(X_train, y_train, tokenizer=tokenizer, model='bert')\n",
    "teacher_dataset_test = tokenize(X_test, y_test, tokenizer=tokenizer, model='bert')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(teacher, teacher_dataset_train, teacher_dataset_test, epochs=5, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BiLSTM(input_dim=vocab_size,\n",
    "               embedding_dim=16,\n",
    "               hidden_dim=16,\n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.5,\n",
    "               num_layers=1)\n",
    "\n",
    "simple_model = SimpleModel(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "simple_dataset_train = tokenize(X_train, y_train, field=field, model='simple')\n",
    "simple_dataset_test = tokenize(X_test, y_test, field=field, model='simple')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(simple_model, simple_dataset_train,simple_dataset_test, epochs=5, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_output_train = get_teacher_output(teacher, teacher_dataset_train)\n",
    "teacher_output_test = get_teacher_output(teacher, teacher_dataset_test)\n",
    "\n",
    "distil_dataset_train = tokenize(X_train, y_train, teacher_output=teacher_output_train, field=field, model='distil')\n",
    "distil_dataset_test = tokenize(X_test, y_test, teacher_output=teacher_output_test, field=field, model='distil')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distil_model = BiLSTM(input_dim=vocab_size,\n",
    "               embedding_dim=16,\n",
    "               hidden_dim=16,\n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.8,\n",
    "               num_layers=1)\n",
    "student = DistilModel(distil_model, alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(student, distil_dataset_train, distil_dataset_test, epochs=5, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}